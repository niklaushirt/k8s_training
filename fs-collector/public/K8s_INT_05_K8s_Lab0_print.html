<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>K8s_INT_05_K8s_Lab0</title>


<style type="text/css">
/*
   This document has been created with Marked.app <http://marked2app.com>
   Please leave this notice in place, along with any additional credits below.
   ---------------------------------------------------------------
   Title: Kult
   Author: Peter Sziebig - @bigpe
   Description: Easy to read
*/

@font-face {
    font-family: "Ubuntu";
    font-style: normal;
    font-weight: 300;
    src: local("Ubuntu Light"), local("Ubuntu-Light"), url("http://themes.googleusercontent.com/static/fonts/ubuntu/v4/WtcvfJHWXKxx4x0kuS1kobO3LdcAZYWl9Si6vvxL-qU.woff") format("woff");
}
@font-face {
    font-family: "Ubuntu";
    font-style: normal;
    font-weight: 400;
    src: local("Ubuntu"), url("http://themes.googleusercontent.com/static/fonts/ubuntu/v4/CGXpU_uR_FUfdeyCjAWgZ-vvDin1pK8aKteLpeZ5c0A.woff") format("woff");
}
@font-face {
    font-family: "Ubuntu";
    font-style: normal;
    font-weight: 500;
    src: local("Ubuntu Medium"), local("Ubuntu-Medium"), url("http://themes.googleusercontent.com/static/fonts/ubuntu/v4/gMhvhm-nVj1086DvGgmzB7O3LdcAZYWl9Si6vvxL-qU.woff") format("woff");
}
@font-face {
    font-family: "Ubuntu";
    font-style: normal;
    font-weight: 700;
    src: local("Ubuntu Bold"), local("Ubuntu-Bold"), url("http://themes.googleusercontent.com/static/fonts/ubuntu/v4/nsLtvfQoT-rVwGTHHnkeJrO3LdcAZYWl9Si6vvxL-qU.woff") format("woff");
}
@font-face {
    font-family: "Ubuntu";
    font-style: italic;
    font-weight: 300;
    src: local("Ubuntu Light Italic"), local("Ubuntu-LightItalic"), url("http://themes.googleusercontent.com/static/fonts/ubuntu/v4/DZ_YjBPqZ88vcZCcIXm6VqfTCPadK0KLfdEfFtGWCYw.woff") format("woff");
}



html {
    font-size: 100%;
}
html, button, input, select, textarea {
    font-family: sans-serif;
}

html, body, button, input, select, textarea {
    color: #57534A;
    font-family: "Ubuntu","Myriad Pro","Myriad",sans-serif;
    font-size: 18px;
	font-weight: 300;
}

body{
    margin: 0 auto;
    padding-top: 20px;
    background-color: #FFFFFF;
}

body, textarea {
    line-height: 1.4;
}


body:after {
    clear: both;
    content: "";
    display: table;
}

body {
    padding-left: 6rem;
    padding-right: 6rem;
    margin-left: auto;
    margin-right: auto;
    max-width: 42rem;
    display: block;
}



h1, h2, h3, dt {
    color: #423F37;
    font-weight: 700;
}

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}
h2, .article-list .article-title {
    font-size: 1.5em;
    margin: 0.83em 0;
}
h3, dt {
    font-size: 1.17em;
    margin: 1em 0;
}
h4 {
    font-size: 1em;
    margin: 1.33em 0;
}
h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}
h6 {
    font-size: 0.75em;
    margin: 2.33em 0;
}


a {
    color: #8DB359;
    cursor: pointer;
    outline: 0 none;
    text-decoration: underline;
}

a:hover {
    outline: 0 none;
    color: #739544;
}


p, pre {
    margin: 1em 0;
}
code, kbd, pre, samp {
    font-family: monospace,serif;
    font-size: 1em;
    margin: 0;
    padding: 0;

}
pre {
    white-space: pre-wrap;
    word-wrap: break-word;
}

pre {
    background-color: #F8F5F0;
    font-size: 0.7rem;
    overflow-x: auto;
    padding: 1.3rem;
    position: relative;
    white-space: pre;
    word-wrap: normal;
}
pre, code, kbd, samp {
    margin: 0;
}
code, kbd, pre, samp {
    font-family: monospace,serif;
}

code {
    color: #423F37;
}


aside {
    display: block;
    float: right;
    width: 390px;
}


b, strong {
    font-weight: bold;
    color: #423F37;
    font-weight: 700;
}

blockquote {
    color: #423F37;
    font-size: 1.25em;
    font-weight: 700;
	margin: 1em 40px;
}

blockquote {
    margin-bottom: 2em;
    margin-top: 2em;
}

figure {
	margin-left: -4.5rem;
    margin-right: -4.5rem;
    margin-bottom: 2em;
    margin-top: 2em;
}


hr {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    border-color: -moz-use-text-color -moz-use-text-color #ECE6DA;
    border-image: none;
    border-style: none none solid;
    border-width: medium medium 1px;
    margin: 3em 6em;
}


img {
    max-width: 100%;
    display: block;
    border: 0 none;
}


ol > li:before {
    color: #423F37;
    content: counter(ol, decimal) ".";
    counter-increment: ol;
    font-weight: 700;
    margin-right: 0.333em;
    position: absolute;
    right: 100%;
}

ul > li:before {
    background-color: #423F37;
    border-radius: 14px 14px 14px 14px;
    content: "";
    height: 6px;
    margin-right: 0.333em;
    margin-top: 0.55em;
    position: absolute;
    right: 100%;
    width: 6px;
}

ol, ul, dl {
    margin-left: 2rem;
    padding: 0;
}
ol {
    counter-reset: ol;
}
li + li, dd + dt {
    margin-top: 0.5em;
}

ul > li {
    position: relative;
}

ol > li {
    position: relative;
}
li {
    list-style: none outside none;
}


figure > figcaption {
    margin-top: 0.5em;
}
small, dd, figcaption {
    color: #A19C91;
    display: block;
    font-size: 0.8rem;
    font-style: italic;
    line-height: 1.2;
}


tbody{display:table-row-group}
tfoot{display:table-footer-group}
table{margin-bottom:2em;font-size: 0.8em;padding:0;border-collapse:collapse;-webkit-box-shadow:1px 1px 2px rgba(0,0,0,.35);width:80%;margin:0 auto 2em auto}
table th,table td{padding:10px 10px 9px;line-height:18px;text-align:left}
table th{
padding-top:9px;!important;text-transform:uppercase;vertical-align:middle}
table td{vertical-align:top;border-top:1px solid #ddd;}
table tbody th{border-top:1px solid #ddd;vertical-align:top}
table{border:1px solid #ddd;border-collapse:separate;*border-collapse:collapse;-webkit-border-radius:4px;-moz-border-radius:4px;border-radius:4px}
table th+th,table td+td,table th+td{border-left:1px solid #ddd}
table thead tr:first-child th:first-child,table tbody tr:first-child td:first-child{-webkit-border-radius:4px 0 0 0;-moz-border-radius:4px 0 0 0;border-radius:4px 0 0 0}
table thead tr:first-child th:last-child,table tbody tr:first-child td:last-child{-webkit-border-radius:0 4px 0 0;-moz-border-radius:0 4px 0 0;border-radius:0 4px 0 0}
table tbody tr:last-child td:first-child{-webkit-border-radius:0 0 0 4px;-moz-border-radius:0 0 0 4px;border-radius:0 0 0 4px}
table tbody tr:last-child td:last-child{-webkit-border-radius:0 0 4px 0;-moz-border-radius:0 0 4px 0;border-radius:0 0 4px 0}
tbody tr:nth-child(odd){background-color:rgba(0,0,0,0.03)}

caption{display:table-caption;font-weight:300;font-size:1.3em;text-transform:uppercase;letter-spacing:2px;word-spacing:.2em;background:rgba(0,0,0,.75);color:#EEE;padding:4px;-webkit-border-radius:4px;margin:4px 0;-webkit-box-shadow:2px 2px 2px rgba(0,0,0,.35)}

/* grey out placeholders */
:-moz-placeholder {
  color: #bfbfbf;
}
::-webkit-input-placeholder {
  color: #bfbfbf;
}


.article-date {
    color: #C7C2B8;
    display: block;
    font-size: 0.8rem;
}
</style>

<style type="text/css">
/**
 * prism.js tomorrow night eighties for JavaScript, CoffeeScript, CSS and HTML
 * Based on https://github.com/chriskempson/tomorrow-theme
 * @author Rose Pritchard
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #ccc;
	background: none;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;

}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #2d2d2d;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.block-comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #999;
}

.token.punctuation {
	color: #ccc;
}

.token.tag,
.token.attr-name,
.token.namespace,
.token.deleted {
	color: #e2777a;
}

.token.function-name {
	color: #6196cc;
}

.token.boolean,
.token.number,
.token.function {
	color: #f08d49;
}

.token.property,
.token.class-name,
.token.constant,
.token.symbol {
	color: #f8c555;
}

.token.selector,
.token.important,
.token.atrule,
.token.keyword,
.token.builtin {
	color: #cc99cd;
}

.token.string,
.token.char,
.token.attr-value,
.token.regex,
.token.variable {
	color: #7ec699;
}

.token.operator,
.token.entity,
.token.url {
	color: #67cdcc;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}

.token.inserted {
	color: green;
}
</style>


</head>

<body>

<p>:course<em>title: K8s</em>101_01 Kubernetes Labs</p>

<p>:course_desc: This course contains the Kubernetes Labs.  </p>

<p>:course_max: 15</p>

<p>:course_auto: no</p>

<p>:button1_label: Task</p>

<p>:button2_label: Hint</p>

<p>:button2_delay: 999999</p>

<p>:button3_label: Complete</p>

<p>:button3_delay: 300</p>

<h4 id="toc_0">Task LabInformation</h4>

<hr>

<h1 id="toc_1">Lab information</h1>

<p>IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Container Service runs Kubernetes clusters which deliver the following:</p>

<ul>
<li>Powerful tools</li>
<li>Intuitive user experience</li>
<li>Built-in security and isolation to enable rapid delivery of secure applications</li>
<li>Cloud services including cognitive capabilities from Watson</li>
<li>Capability to manage dedicated cluster resources for both stateless applications and stateful workloads</li>
</ul>

<h1 id="toc_2">Lab overview</h1>

<ul>
<li><p>Lab 0: (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine.</p></li>
<li><p>Lab 1: This lab walks through creating and deploying a simple &quot;guestbook&quot; app written in Go as a net/http Server and accessing it.</p></li>
<li><p>Lab 2: Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Container Service</p></li>
<li><p>Lab 3: Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices.</p></li>
<li><p>Lab 4: How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention.</p></li>
<li><p>Lab D: Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs. </p></li>
</ul>

<h4 id="toc_3">Hint LabInformation</h4>

<p>No hint available</p>

<h4 id="toc_4">Complete LabInformation</h4>

<blockquote>
<p>Confirm Lab Information complete</p>
</blockquote>

<hr>

<h4 id="toc_5">Task Kubernetes<em>Lab</em>0</h4>

<hr>

<h1 id="toc_6">Download the Workshop Source Code</h1>

<p>Repo <code>guestbook</code> has the application that we&#39;ll be deploying.
While we&#39;re not going to build it we will use the deployment configuration files from that repo.
Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout
functionality later. All the configuration files we use are under the directory guestbook/v1.</p>

<p>Repo <code>kube101</code> contains the step by step instructions to run the workshop.</p>

<div><pre><code class="language-none">$ git clone https://github.com/IBM/guestbook.git
$ git clone https://github.com/niklaushirt/kube101.git</code></pre></div>

<h1 id="toc_7">Configure and check your cluster</h1>

<ul>
<li><p>Run the following connamd, and set the <code>KUBECONFIG</code>
environment variable based on the output of the command. This will
make your <code>kubectl</code> client point to your new Kubernetes cluster. </p>

<div><pre><code class="language-none">$ ibmcloud ks cluster-config &lt;name-of-cluster&gt;

$ export KUBECONFIG=/Users/$USER/.bluemix/plugins/container-service/clusters/mycluster/kube-config-mil01-mycluster.yml</code></pre></div></li>
<li><p>Verify kubectl can communicate with your cluster.</p>

<div><pre><code class="language-none">$ kubectl cluster-info   

Kubernetes master is running at https://c3.lon04.containers.cloud.ibm.com:23520
KubeDNS is running at https://c3.lon04.containers.cloud.ibm.com:23520/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://c3.lon04.containers.cloud.ibm.com:23520/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
Metrics-server is running at https://c3.lon04.containers.cloud.ibm.com:23520/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy</code></pre></div></li>
<li><p>Run the following two commands to check if your cluster is ready</p>

<div><pre><code class="language-none">$ kubectl get nodes  
                                                          master
NAME            STATUS   ROLES    AGE   VERSION
10.144.180.13   Ready    &lt;none&gt;   27m   v1.12.7+IKS
</code></pre></div></li>
</ul>

<h4 id="toc_8">Hint Kubernetes<em>Lab</em>0</h4>

<p>No hint available</p>

<h4 id="toc_9">Complete Kubernetes<em>Lab</em>0</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>0 complete</p>
</blockquote>

<h4 id="toc_10">Task Kubernetes<em>Lab</em>0_kubectl</h4>

<hr>

<h2 id="toc_11">Lab 0. Get to know kubectl</h2>

<p>Learn how to use the <code>kubectl</code> command line.</p>

<p>Open your terminal:</p>

<p>On your Mac you can hit Command + Space and type Terminal.</p>

<p>Type <code>kubectl</code> and you will get the list of all available commands.</p>

<div><pre><code class="language-none">$ kubectl

kubectl controls the Kubernetes cluster manager.

Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/

Basic Commands (Beginner):
  create         Create a resource from a file or from stdin.
  expose         Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
  run            Run a particular image on the cluster
  set            Set specific features on objects

Basic Commands (Intermediate):
  explain        Documentation of resources
  get            Display one or many resources
  edit           Edit a resource on the server
  delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector
...
</code></pre></div>

<p>They allow you to work with Kubernetes objects.</p>

<p>The most important ones are:</p>

<ul>
<li><code>create</code>        Create a resource from a file or from stdin.</li>
<li><code>delete</code>         Delete resources by filenames, stdin, resources and names, or by resources </li>
<li><code>get</code>            Display one or many resources</li>
<li><code>describe</code>       Show details of a specific resource or group of resources</li>
</ul>

<p>For example you can get the nodes in your cluster by typing</p>

<div><pre><code class="language-none">$ kubectl get nodes</code></pre></div>

<h4 id="toc_12">Hint Kubernetes<em>Lab</em>0_kubectl</h4>

<p>No hint available</p>

<h4 id="toc_13">Complete Kubernetes<em>Lab</em>0_kubectl</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>0_kubectl complete</p>
</blockquote>

<h4 id="toc_14">Task Kubernetes<em>Lab</em>0_yaml</h4>

<hr>

<h2 id="toc_15">Lab 0. Get to know yaml</h2>

<p>YAML (&quot;YAML Ain&#39;t Markup Language&quot;) is a human-readable data serialization language. It is commonly used for configuration files, but could be used in many applications where data is being stored (e.g. debugging output) or transmitted (e.g. document headers). </p>

<p>The YAML format is generally used to describe Kubernetes objects.</p>

<ul>
<li>Whitespace indentation is used to denote structure; however, tab characters are never allowed as indentation.</li>
<li>Comments begin with the number sign (#)</li>
<li>List members are denoted by a leading hyphen (-) with one member per line</li>
<li>Strings are ordinarily unquoted, but may be enclosed in double-quotes (&quot;), or single-quotes (&#39;).</li>
</ul>

<p><strong>guestbook-deployment.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: apps/v1
kind: Deployment
metadata:
  name: guestbook-v1
  labels:
    app: guestbook
    version: &quot;1.0&quot;
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
  template:
    metadata:
      labels:
        app: guestbook
        version: &quot;1.0&quot;
    spec:
      containers:
      - name: guestbook
        image: ibmcom/guestbook:v1
        ports:
        - name: http-server
          containerPort: 3000</code></pre></div>

<p>In this example, the structure <code>metadata</code> contains <code>name</code> and <code>labels</code>. <code>labels</code> in turn contains <code>app</code> and <code>version</code>.</p>

<h4 id="toc_16">Hint Kubernetes<em>Lab</em>0_yaml</h4>

<p>No hint available</p>

<h4 id="toc_17">Complete Kubernetes<em>Lab</em>0_yaml</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>0_yaml complete</p>
</blockquote>

<h4 id="toc_18">Task Kubernetes<em>Lab</em>1_deploy</h4>

<hr>

<h2 id="toc_19">Lab 1. Set up and deploy your first application</h2>

<p>Learn how to deploy an application to a Kubernetes cluster hosted within
the IBM Container Service.</p>

<p>Once your client is configured, you are ready to deploy your first application, <code>guestbook</code>.</p>

<h3 id="toc_20">1. Deploy your application</h3>

<p>In this part of the lab we will deploy an application called <code>guestbook</code>
that has already been built and uploaded to DockerHub under the name
<code>ibmcom/guestbook:v1</code>.</p>

<ol>
<li><p>Start by running <code>guestbook</code></p>

<div><pre><code class="language-none">$ kubectl run guestbook --image=ibmcom/guestbook:v1</code></pre></div>

<p>This action will take a bit of time. To check the status of the running application,
you can use <code>$ kubectl get pods</code>.</p>

<p>You should see output similar to the following:</p>

<div><pre><code class="language-none">$ kubectl get pods
NAME                          READY     STATUS              RESTARTS   AGE
guestbook-59bd679fdc-bxdg7    0/1       ContainerCreating   0          1m</code></pre></div>

<p>Eventually, the status should show up as <code>Running</code>.</p>

<div><pre><code class="language-none">$ kubectl get pods
NAME                          READY     STATUS              RESTARTS   AGE
guestbook-59bd679fdc-bxdg7    1/1       Running             0          1m</code></pre></div>

<p>The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods.</p></li>
<li><p>Once the status reads <code>Running</code>, we need to expose that deployment as a service so we can access it through the IP of the worker nodes.
The <code>guestbook</code> application listens on port 3000.  Run:</p>

<div><pre><code class="language-none">$ kubectl expose deployment guestbook --type=&quot;NodePort&quot; --port=3000
service &quot;guestbook&quot; exposed</code></pre></div></li>
<li><p>To find the port used on that worker node, examine your new service:</p>

<div><pre><code class="language-none">$ kubectl get service guestbook
NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
guestbook   NodePort   10.10.10.253   &lt;none&gt;        3000:31208/TCP   1m</code></pre></div>

<p>We can see that our <code>&lt;nodeport&gt;</code> is <code>31208</code>. We can see in the output the port mapping from 3000 inside 
the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, 
and could be different for you.</p></li>
<li><p><code>guestbook</code> is now running on your cluster, and exposed to the internet. We need to find out where it is accessible.
The worker nodes running in the container service get external IP addresses.
Run <code>$ ibmcloud cs workers &lt;name-of-cluster&gt;</code>, and note the public IP listed on the <code>&lt;public-IP&gt;</code> line.</p>

<div><pre><code class="language-none">$ ibmcloud cs workers &lt;name-of-cluster&gt;
OK
ID                                                 Public IP        Private IP     Machine Type   State    Status   Zone    Version  
kube-hou02-pa1e3ee39f549640aebea69a444f51fe55-w1   173.193.99.136   10.76.194.30   free           normal   Ready    hou02   1.5.6_1500*</code></pre></div>

<p>We can see that our <code>&lt;public-IP&gt;</code> is <code>173.193.99.136</code>.</p></li>
<li><p>Now that you have both the address and the port, you can now access the application in the web browser
at <code>&lt;public-IP&gt;:&lt;nodeport&gt;</code>. In the example case this is <code>173.193.99.136:31208</code>.</p></li>
</ol>

<p>Congratulations, you&#39;ve now deployed an application to Kubernetes!</p>

<p>We will be using this deployment in the next lab of this course (Lab2).</p>

<p>You should now go back up to the root of the repository in preparation
for the next lab: <code>$ cd ..</code>.</p>

<h4 id="toc_21">Hint Kubernetes<em>Lab</em>1_deploy</h4>

<p>No hint available</p>

<h4 id="toc_22">Complete Kubernetes<em>Lab</em>1_deploy</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>1 complete</p>
</blockquote>

<h4 id="toc_23">Task Kubernetes<em>Lab</em>2_Scale</h4>

<hr>

<h2 id="toc_24">Lab 2: Scale and Update Deployments</h2>

<p>In this lab, you&#39;ll learn how to update the number of instances
a deployment has and how to safely roll out an update of your application
on Kubernetes. </p>

<p>For this lab, you need a running deployment of the <code>guestbook</code> application
from the previous lab. If you deleted it, recreate it using:</p>

<div><pre><code class="language-none">$ kubectl run guestbook --image=ibmcom/guestbook:v1</code></pre></div>

<h3 id="toc_25">Scale apps with replicas</h3>

<p>A <em>replica</em> is a copy of a pod that contains a running service. By having
multiple replicas of a pod, you can ensure your deployment has the available
resources to handle increasing load on your application.</p>

<ol>
<li><p><code>kubectl</code> provides a <code>scale</code> subcommand to change the size of an
existing deployment. Let&#39;s increase our capacity from a single running instance of
<code>guestbook</code> up to 10 instances:</p>

<div><pre><code class="language-none">$ kubectl scale --replicas=10 deployment guestbook
deployment &quot;guestbook&quot; scaled</code></pre></div>

<p>Kubernetes will now try to make reality match the desired state of
10 replicas by starting 9 new pods with the same configuration as
the first.</p></li>
<li><p>To see your changes being rolled out, you can run:
<code>kubectl rollout status deployment guestbook</code>.</p>

<p>The rollout might occur so quickly that the following messages might
<em>not</em> display:</p>

<div><pre><code class="language-none">$ kubectl rollout status deployment guestbook
Waiting for rollout to finish: 1 of 10 updated replicas are available...
Waiting for rollout to finish: 2 of 10 updated replicas are available...
Waiting for rollout to finish: 3 of 10 updated replicas are available...
Waiting for rollout to finish: 4 of 10 updated replicas are available...
Waiting for rollout to finish: 5 of 10 updated replicas are available...
Waiting for rollout to finish: 6 of 10 updated replicas are available...
Waiting for rollout to finish: 7 of 10 updated replicas are available...
Waiting for rollout to finish: 8 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
deployment &quot;guestbook&quot; successfully rolled out</code></pre></div></li>
<li><p>Once the rollout has finished, ensure your pods are running by using:
<code>kubectl get pods</code>.</p>

<p>You should see output listing 10 replicas of your deployment:</p>

<div><pre><code class="language-none">$ kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE
guestbook-562211614-1tqm7   1/1       Running   0          1d
guestbook-562211614-1zqn4   1/1       Running   0          2m
guestbook-562211614-5htdz   1/1       Running   0          2m
guestbook-562211614-6h04h   1/1       Running   0          2m
guestbook-562211614-ds9hb   1/1       Running   0          2m
guestbook-562211614-nb5qp   1/1       Running   0          2m
guestbook-562211614-vtfp2   1/1       Running   0          2m
guestbook-562211614-vz5qw   1/1       Running   0          2m
guestbook-562211614-zksw3   1/1       Running   0          2m
guestbook-562211614-zsp0j   1/1       Running   0          2m</code></pre></div></li>
</ol>

<p><strong>Tip:</strong> Another way to improve availability is to 
 <a href="https://console.bluemix.net/docs/containers/cs_planning.html#cs_planning_cluster_config">add clusters and regions</a>
to your deployment, as shown in the following diagram:</p>

<p><img src="https://raw.githubusercontent.com/niklaushirt/kube101/master/workshop/images/cluster_ha_roadmap.png" alt="HA with more clusters and regions"></p>

<h4 id="toc_26">Hint Kubernetes<em>Lab</em>2_Scale</h4>

<p>No hint available</p>

<h4 id="toc_27">Complete Kubernetes<em>Lab</em>2_Scale</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>2_Scale complete</p>
</blockquote>

<h4 id="toc_28">Task Kubernetes<em>Lab</em>2_Update</h4>

<hr>

<h3 id="toc_29">Update and roll back apps</h3>

<p>Kubernetes allows you to do rolling upgrade of your application to a new
container image. This allows you to easily update the running image and also allows you to
easily undo a rollout if a problem is discovered during or after deployment.</p>

<p>In the previous lab, we used an image with a <code>v1</code> tag. For our upgrade
we&#39;ll use the image with the <code>v2</code> tag.</p>

<p>To update and roll back:
1. Using <code>kubectl</code>, you can now update your deployment to use the
   <code>v2</code> image. <code>kubectl</code> allows you to change details about existing
   resources with the <code>set</code> subcommand. We can use it to change the
   image being used.</p>

<p><code>$ kubectl set image deployment/guestbook guestbook=ibmcom/guestbook:v2</code></p>

<p>Note that a pod could have multiple containers, each with its own name.
   Each image can be changed individually or all at once by referring to the name.
   In the case of our <code>guestbook</code> Deployment, the container name is also <code>guestbook</code>.
    Multiple containers can be updated at the same time.
   (<a href="https://kubernetes.io/docs/user-guide/kubectl/kubectl_set_image/">More information</a>.)</p>

<ol>
<li><p>Run <code>kubectl rollout status deployment/guestbook</code> to check the status of
the rollout. The rollout might occur so quickly that the following messages
might <em>not</em> display:</p>

<div><pre><code class="language-none">$ kubectl rollout status deployment/guestbook
Waiting for rollout to finish: 2 out of 10 new replicas have been updated...
Waiting for rollout to finish: 3 out of 10 new replicas have been updated...
Waiting for rollout to finish: 3 out of 10 new replicas have been updated...
Waiting for rollout to finish: 3 out of 10 new replicas have been updated...
Waiting for rollout to finish: 4 out of 10 new replicas have been updated...
Waiting for rollout to finish: 4 out of 10 new replicas have been updated...
Waiting for rollout to finish: 4 out of 10 new replicas have been updated...
Waiting for rollout to finish: 4 out of 10 new replicas have been updated...
Waiting for rollout to finish: 4 out of 10 new replicas have been updated...
Waiting for rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for rollout to finish: 9 out of 10 new replicas have been updated...
Waiting for rollout to finish: 9 out of 10 new replicas have been updated...
Waiting for rollout to finish: 9 out of 10 new replicas have been updated...
Waiting for rollout to finish: 1 old replicas are pending termination...
Waiting for rollout to finish: 1 old replicas are pending termination...
Waiting for rollout to finish: 1 old replicas are pending termination...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
deployment &quot;guestbook&quot; successfully rolled out</code></pre></div></li>
<li><p>Test the application as before, by accessing <code>&lt;public-IP&gt;:&lt;nodeport&gt;</code> 
in the browser to confirm your new code is active.</p>

<p>Remember, to get the &quot;nodeport&quot; and &quot;public-ip&quot; use:</p>

<p><code>$ kubectl describe service guestbook</code>
and
<code>$ ibmcloud cs workers &lt;name-of-cluster&gt;</code></p>

<p>To verify that you&#39;re running &quot;v2&quot; of guestbook, look at the title of the page,
it should now be <code>Guestbook - v2</code></p></li>
<li><p>If you want to undo your latest rollout, use:
<code>
$ kubectl rollout undo deployment guestbook
deployment &quot;guestbook&quot;
</code></p>

<p>You can then use <code>kubectl rollout status deployment/guestbook</code> to see
the status.</p></li>
<li><p>When doing a rollout, you see references to <em>old</em> replicas and <em>new</em> replicas.
The <em>old</em> replicas are the original 10 pods deployed when we scaled the application.
The <em>new</em> replicas come from the newly created pods with the different image.
All of these pods are owned by the Deployment.
The deployment manages these two sets of pods with a resource called a ReplicaSet.
We can see the guestbook ReplicaSets with:</p>

<div><pre><code class="language-none">$ kubectl get replicasets -l run=guestbook
NAME                   DESIRED   CURRENT   READY     AGE
guestbook-5f5548d4f    10        10        10        21m
guestbook-768cc55c78   0         0         0         3h</code></pre></div></li>
</ol>

<p>Before we continue, let&#39;s delete the application so we can learn about
a different way to achieve the same results:</p>

<p>To remove the deployment, use <code>kubectl delete deployment guestbook</code>.</p>

<p>To remove the service, use <code>kubectl delete service guestbook</code>.</p>

<p>Congratulations! You deployed the second version of the app. 
Lab 2 is now complete.</p>

<h4 id="toc_30">Hint Kubernetes<em>Lab</em>2_Update</h4>

<p>No hint available</p>

<h4 id="toc_31">Complete Kubernetes<em>Lab</em>2_Update</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>2_Update complete</p>
</blockquote>

<h4 id="toc_32">Task Kubernetes<em>Lab</em>3<em>Scale</em>1</h4>

<hr>

<h2 id="toc_33">Lab 3: Scale and update apps natively, building multi-tier applications.</h2>

<p>In this lab you&#39;ll learn how to deploy the same guestbook application we
deployed in the previous labs, however, instead of using the <code>kubectl</code>
command line helper functions we&#39;ll be deploying the application using
configuration files. The configuration file mechanism allows you to have more
fine-grained control over all of resources being created within the
Kubernetes cluster.</p>

<p>Before we work with the application we need to clone a github repo:</p>

<div><pre><code class="language-none">$ git clone https://github.com/IBM/guestbook.git</code></pre></div>

<p>This repo contains multiple versions of the guestbook application
as well as the configuration files we&#39;ll use to deploy the pieces of the application.</p>

<p>Change directory by running the command <code>cd guestbook</code>. You will find all the
configurations files for this exercise under the directory <code>v1</code>.</p>

<h4 id="toc_34">Hint Kubernetes<em>Lab</em>3<em>Scale</em>1</h4>

<p>No hint available</p>

<h4 id="toc_35">Complete Kubernetes<em>Lab</em>3<em>Scale</em>1</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Scale</em>1 complete</p>
</blockquote>

<h4 id="toc_36">Task Kubernetes<em>Lab</em>3<em>Scale</em>2</h4>

<hr>

<h3 id="toc_37">Scale apps natively</h3>

<p>Kubernetes can deploy an individual pod to run an application but when you
need to scale it to handle a large number of requests a <code>Deployment</code> is the
resource you want to use.
A Deployment manages a collection of similar pods. When you ask for a specific number of replicas
the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times.</p>

<p>Every Kubernetes object we create should provide two nested object fields
that govern the object’s configuration: the object <code>spec</code> and the object
<code>status</code>. Object <code>spec</code> defines the desired state, and object <code>status</code>
contains Kubernetes system provided information about the actual state of the
resource. As described before, Kubernetes will attempt to reconcile
your desired state with the actual state of the system.</p>

<p>For Object that we create we need to provide the <code>apiVersion</code> you are using
to create the object, <code>kind</code> of the object we are creating and the <code>metadata</code>
about the object such as a <code>name</code>, set of <code>labels</code> and optionally <code>namespace</code>
that this object should belong.</p>

<p>Consider the following deployment configuration for guestbook application</p>

<p><strong>guestbook-deployment.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: apps/v1
kind: Deployment
metadata:
  name: guestbook
  labels:
    app: guestbook
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
  template:
    metadata:
      labels:
        app: guestbook
    spec:
      containers:
      - name: guestbook
        image: ibmcom/guestbook:v1
        ports:
        - name: http-server
          containerPort: 3000</code></pre></div>

<p>The above configuration file create a deployment object named &#39;guestbook&#39;
with a pod containing a single container running the image
<code>ibmcom/guestbook:v1</code>.  Also the configuration specifies replicas set to 3
and Kubernetes tries to make sure that at least three active pods are running at
all times.</p>

<ul>
<li><p>Create guestbook deployment</p>

<p>To create a Deployment using this configuration file we use the
following command:</p>

<div><pre><code class="language-console">$ cd guestbook/v1/
$ kubectl create -f guestbook-deployment.yaml
deployment &quot;guestbook&quot; created</code></pre></div></li>
<li><p>List the pod with label app=guestbook</p>

<p>We can then list the pods it created by listing all pods that
have a label of &quot;app&quot; with a value of &quot;guestbook&quot;. This matches
the labels defined above in the yaml file in the
<code>spec.template.metadata.labels</code> section.</p>

<div><pre><code class="language-none">$ kubectl get pods -l app=guestbook</code></pre></div></li>
</ul>

<p>When you change the number of replicas in the configuration, Kubernetes will
try to add, or remove, pods from the system to match your request. </p>

<p>Open the deployment file we used to create the Deployment in your preferred editor to make changes. 
You&#39;ll notice that there are a lot more fields in this version than the original yaml
file we used. This is because it contains all of the properties about the
Deployment that Kubernetes knows about, not just the ones we chose to
specify when we create it. </p>

<p>You should use the following command to make the change
effective when you have finished editing the yaml file.</p>

<div><pre><code class="language-none">   $ kubectl apply -f guestbook-deployment.yaml</code></pre></div>

<p>This will ask Kubernetes to &quot;diff&quot; our yaml file with the current state
of the Deployment and apply just those changes.</p>

<h4 id="toc_38">Hint Kubernetes<em>Lab</em>3<em>Scale</em>2</h4>

<p>No hint available</p>

<h4 id="toc_39">Complete Kubernetes<em>Lab</em>3<em>Scale</em>2</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Scale</em>2 complete</p>
</blockquote>

<h4 id="toc_40">Task Kubernetes<em>Lab</em>3<em>Scale</em>3</h4>

<hr>

<p>We can now define a Service object to expose the deployment to external
clients.</p>

<p><strong>guestbook-service.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: v1
kind: Service
metadata:
  name: guestbook
  labels:
    app: guestbook
spec:
  ports:
  - port: 3000
    targetPort: http-server
  selector:
    app: guestbook
  type: LoadBalancer</code></pre></div>

<p>The above configuration creates a Service resource named guestbook. A Service
can be used to create a network path for incoming traffic to your running
application.  In this case, we are setting up a route from port 3000 on the
cluster to the &quot;http-server&quot; port on our app, which is port 3000 per the
Deployment container spec.</p>

<ul>
<li><p>Let us now create the guestbook service using the same type of command
we used when we created the Deployment:</p>

<div><pre><code class="language-none">$ kubectl create -f guestbook-service.yaml </code></pre></div></li>
<li><p>Test guestbook app using a browser of your choice using the url
<code>&lt;your-cluster-ip&gt;:&lt;node-port&gt;</code></p>

<p>Remember, to get the <code>nodeport</code> and <code>public-ip</code> use:</p>

<div><pre><code class="language-none">$ kubectl describe service guestbook</code></pre></div>

<p>and</p>

<div><pre><code class="language-none">$ ibmcloud cs workers &lt;name-of-cluster&gt;</code></pre></div></li>
</ul>

<h4 id="toc_41">Hint Kubernetes<em>Lab</em>3<em>Scale</em>3</h4>

<p>No hint available</p>

<h4 id="toc_42">Complete Kubernetes<em>Lab</em>3<em>Scale</em>3</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Scale</em>3 complete</p>
</blockquote>

<h4 id="toc_43">Task Kubernetes<em>Lab</em>3<em>Backend</em>1</h4>

<hr>

<h3 id="toc_44">Connect to a back-end service</h3>

<p>If you look at the guestbook source code, under the <code>guestbook/v1/guestbook</code>
directory, you&#39;ll notice that it is written to support a variety of data
stores. By default it will keep the log of guestbook entries in memory.
That&#39;s ok for testing purposes, but as you get into a more &quot;real&quot; environment
where you scale your application that model will not work because
based on which instance of the application the user is routed to they&#39;ll see
very different results.</p>

<p>To solve this we need to have all instances of our app share the same data
store - in this case we&#39;re going to use a redis database that we deploy to our
cluster. This instance of redis will be defined in a similar manner to the guestbook.</p>

<h3 id="toc_45">Deploy Redis Master</h3>

<p><strong>redis-master-deployment.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
  template:
    metadata:
      labels:
        app: redis
        role: master
    spec:
      containers:
      - name: redis-master
        image: redis:2.8.23
        ports:
        - name: redis-server
          containerPort: 6379</code></pre></div>

<p>This yaml creates a redis database in a Deployment named &#39;redis-master&#39;.
It will create a single instance, with replicas set to 1, and the guestbook app instances
will connect to it to persist data, as well as read the persisted data back.
The image running in the container is &#39;redis:2.8.23&#39; and exposes the standard redis port 6379.</p>

<ul>
<li><p>Create a redis Deployment, like we did for guestbook:</p>

<div><pre><code class="language-none">$ kubectl create -f redis-master-deployment.yaml</code></pre></div></li>
<li><p>Check to see that redis server pod is running:</p>

<div><pre><code class="language-none">$ kubectl get pods -l app=redis,role=master
NAME                 READY     STATUS    RESTARTS   AGE
redis-master-q9zg7   1/1       Running   0          2d</code></pre></div></li>
<li><p>Let us test the redis standalone:</p>

<p><code>$ kubectl exec -it redis-master-q9zg7 redis-cli</code></p>

<p>The kubectl exec command will start a secondary process in the specified
container. In this case we&#39;re asking for the &quot;redis-cli&quot; command to be
executed in the container named &quot;redis-master-q9zg7&quot;.  When this process
ends the &quot;kubectl exec&quot; command will also exit but the other processes in
the container will not be impacted.</p>

<p>Once in the container we can use the &quot;redis-cli&quot; command to make sure the
redis database is running properly, or to configure it if needed.</p>

<div><pre><code class="language-none">redis-cli&gt; ping
PONG
redis-cli&gt; exit</code></pre></div></li>
</ul>

<h4 id="toc_46">Hint Kubernetes<em>Lab</em>3<em>Backend</em>1</h4>

<p>No hint available</p>

<h4 id="toc_47">Complete Kubernetes<em>Lab</em>3<em>Backend</em>1</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Backend</em>1 complete</p>
</blockquote>

<h4 id="toc_48">Task Kubernetes<em>Lab</em>3<em>Backend</em>2</h4>

<hr>

<h3 id="toc_49">Expose Redis Master</h3>

<p>Now we need to expose the <code>redis-master</code> Deployment as a Service so that the
guestbook application can connect to it through DNS lookup. </p>

<p><strong>redis-master-service.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: master</code></pre></div>

<p>This creates a Service object named &#39;redis-master&#39; and configures it to target
port 6379 on the pods selected by the selectors &quot;app=redis&quot; and &quot;role=master&quot;.</p>

<ul>
<li><p>Create the service to access redis master:</p>

<p><code>$ kubectl create -f redis-master-service.yaml</code></p></li>
</ul>

<ul>
<li><p>Restart guestbook so that it will find the redis service to use database:</p>

<div><pre><code class="language-none">$ kubectl delete deploy guestbook-v1
$ kubectl create -f guestbook-deployment.yaml</code></pre></div></li>
<li><p>Test guestbook app using a browser of your choice using the url:
<code>&lt;your-cluster-ip&gt;:&lt;node-port&gt;</code></p></li>
</ul>

<p>You can see now that if you open up multiple browsers and refresh the page
to access the different copies of guestbook that they all have a consistent state.
All instances write to the same backing persistent storage, and all instances
read from that storage to display the guestbook entries that have been stored.</p>

<p>We have our simple 3-tier application running but we need to scale the
application if traffic increases. Our main bottleneck is that we only have
one database server to process each request coming though guestbook. One
simple solution is to separate the reads and write such that they go to
different databases that are replicated properly to achieve data consistency.</p>

<p><img src="https://raw.githubusercontent.com/niklaushirt/kube101/master/workshop/images/Master.png" alt="rw_to_master"></p>

<h4 id="toc_50">Hint Kubernetes<em>Lab</em>3<em>Backend</em>2</h4>

<p>No hint available</p>

<h4 id="toc_51">Complete Kubernetes<em>Lab</em>3<em>Backend</em>2</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Backend</em>2 complete</p>
</blockquote>

<h4 id="toc_52">Task Kubernetes<em>Lab</em>3<em>Backend</em>3</h4>

<hr>

<h3 id="toc_53">Deploy Redis Slave</h3>

<p>Create a deployment named &#39;redis-slave&#39; that can talk to redis database to
manage data reads. In order to scale the database we use the pattern where
we can scale the reads using redis slave deployment which can run several
instances to read. Redis slave deployments is configured to run two replicas.</p>

<p><img src="https://raw.githubusercontent.com/niklaushirt/kube101/master/workshop/images/Master-Slave.png" alt="w_to_master-r_to_slave"></p>

<p><strong>redis-slave-deployment.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
  template:
    metadata:
      labels:
        app: redis
        role: slave
    spec:
      containers:
      - name: redis-slave
        image: kubernetes/redis-slave:v2
        ports:
        - name: redis-server
          containerPort: 6379</code></pre></div>

<ul>
<li><p>Create the pod  running redis slave deployment.
<code>$ kubectl create -f redis-slave-deployment.yaml</code></p>

<ul>
<li>Check if all the slave replicas are running</li>
</ul></li>
</ul>

<div><pre><code class="language-none">$ kubectl get pods -l app=redis,role=slave
NAME                READY     STATUS    RESTARTS   AGE
redis-slave-kd7vx   1/1       Running   0          2d
redis-slave-wwcxw   1/1       Running   0          2d</code></pre></div>

<ul>
<li>And then go into one of those pods and look at the database to see
that everything looks right:</li>
</ul>

<div><pre><code class="language-none">$ kubectl exec -it redis-slave-kd7vx  redis-cli

127.0.0.1:6379&gt; keys *
1) &quot;guestbook&quot;
127.0.0.1:6379&gt; lrange guestbook 0 10
1) &quot;hello world&quot;
2) &quot;welcome to the Kube workshop&quot;
127.0.0.1:6379&gt; exit
</code></pre></div>

<h4 id="toc_54">Hint Kubernetes<em>Lab</em>3<em>Backend</em>3</h4>

<p>No hint available</p>

<h4 id="toc_55">Complete Kubernetes<em>Lab</em>3<em>Backend</em>3</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Backend</em>3 complete</p>
</blockquote>

<h4 id="toc_56">Task Kubernetes<em>Lab</em>3<em>Backend</em>4</h4>

<hr>

<h3 id="toc_57">Expose Redis Slave</h3>

<p>Deploy redis slave service so we can access it by DNS name. Once redeployed,
the application will send &quot;read&quot; operations to the <code>redis-slave</code> pods while
&quot;write&quot; operations will go to the <code>redis-master</code> pods.</p>

<p><strong>redis-slave-service.yaml</strong></p>

<div><pre><code class="language-none">apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: slave</code></pre></div>

<ul>
<li>Create the service to access redis slaves.
<code>$ kubectl create -f redis-slave-service.yaml</code></li>
</ul>

<ul>
<li><p>Restart guestbook so that it will find the slave service to read from.
<code>
$ kubectl delete deploy guestbook-v1
$ kubectl create -f guestbook-deployment.yaml
</code></p></li>
<li><p>Test guestbook app using a browser of your choice using the url <code>&lt;your-cluster-ip&gt;:&lt;node-port&gt;</code>.</p></li>
</ul>

<p>That&#39;s the end of the lab. Now let&#39;s clean-up our environment:</p>

<div><pre><code class="language-none">$ kubectl delete -f guestbook-deployment.yaml
$ kubectl delete -f guestbook-service.yaml
$ kubectl delete -f redis-slave-service.yaml
$ kubectl delete -f redis-slave-deployment.yaml 
$ kubectl delete -f redis-master-service.yaml 
$ kubectl delete -f redis-master-deployment.yaml</code></pre></div>

<h4 id="toc_58">Hint Kubernetes<em>Lab</em>3<em>Backend</em>4</h4>

<p>No hint available</p>

<h4 id="toc_59">Complete Kubernetes<em>Lab</em>3<em>Backend</em>4</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>3<em>Backend</em>4 complete</p>
</blockquote>

<h4 id="toc_60">Task Kubernetes<em>Lab</em>4</h4>

<hr>

<h1 id="toc_61">*** OPTIONAL ***</h1>

<h2 id="toc_62">1. Check the health of apps</h2>

<p>Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs.</p>

<p>Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers.</p>

<p>In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the <code>/healthz</code> returns a <code>200</code> response and will fail afterward. Kubernetes will automatically restart the service.  </p>

<ol>
<li><p>Open the <code>healthcheck.yml</code> file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods:</p>

<ol>
<li><p>Update the details for the image in your private registry namespace:</p>

<div><pre><code class="language-none">image: &quot;ibmcom/guestbook:v2&quot;</code></pre></div></li>
<li><p>Note the HTTP liveness probe that checks the health of the container every five seconds.</p>

<div><pre><code class="language-none">livenessProbe:
          httpGet:
            path: /healthz
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5</code></pre></div></li>
<li><p>In the <strong>Service</strong> section, note the <code>NodePort</code>. Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072.</p></li>
</ol></li>
<li><p>Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see:</p>

<div><pre><code class="language-none">kubectl apply -f healthcheck.yml</code></pre></div>

<p>Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower.</p></li>
<li><p>Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node:</p>

<div><pre><code class="language-none">ibmcloud cs workers &lt;cluster-name&gt;</code></pre></div>

<p>In a browser, you&#39;ll see a success message. If you do not see this text, don&#39;t worry. This app is designed to go up and down.</p>

<p>For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app.</p></li>
<li><p>Launch your Kubernetes dashboard:</p>

<ol>
<li><p>Get your credentials for Kubernetes.</p>

<div><pre><code class="language-none">kubectl config view -o jsonpath=&#39;{.users[0].user.auth-provider.config.id-token}&#39;</code></pre></div></li>
<li><p>Copy the <strong>id-token</strong> value that is shown in the output.     </p></li>
<li><p>Set the proxy with the default port number.</p>

<div><pre><code class="language-none">kubectl proxy</code></pre></div>

<p>Output:</p>

<div><pre><code class="language-none">Starting to serve on 127.0.0.1:8001</code></pre></div></li>
<li><p>Sign in to the dashboard.</p>

<ol>
<li>Open the following URL in a web browser.</li>
</ol>

<div><pre><code class="language-none"> http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code></pre></div>

<ol>
<li><p>In the sign-on page, select the <strong>Token</strong> authentication method.</p></li>
<li><p>Then, paste the <strong>id-token</strong> value that you previously copied into the <strong>Token</strong> field and click <strong>SIGN IN</strong>.</p></li>
</ol></li>
</ol>

<p>In the <strong>Workloads</strong> tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the <strong>Pods</strong> section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod.</p></li>
<li><p>Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created.</p>

<p><code>kubectl delete -f healthcheck.yml</code></p></li>
</ol>

<ol>
<li>When you are done exploring the Kubernetes dashboard, in your CLI, enter <code>CTRL+C</code> to exit the <code>proxy</code> command.</li>
</ol>

<h4 id="toc_63">Hint Kubernetes<em>Lab</em>4</h4>

<p>No hint available</p>

<h4 id="toc_64">Complete Kubernetes<em>Lab</em>4</h4>

<blockquote>
<p>Confirm Kubernetes<em>Lab</em>4 complete</p>
</blockquote>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>


</body>

</html>
